{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dc31e6b8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-19T18:25:50.408478Z",
     "iopub.status.busy": "2025-10-19T18:25:50.408160Z",
     "iopub.status.idle": "2025-10-19T18:27:17.826222Z",
     "shell.execute_reply": "2025-10-19T18:27:17.825220Z"
    },
    "executionInfo": {
     "elapsed": 13039,
     "status": "ok",
     "timestamp": 1760821476058,
     "user": {
      "displayName": "Otávio Leite",
      "userId": "01516780190388766182"
     },
     "user_tz": 180
    },
    "id": "mqLj2t40IZBZ",
    "outputId": "63a05a72-ce49-4b6d-9be3-d14337562594",
    "papermill": {
     "duration": 87.425203,
     "end_time": "2025-10-19T18:27:17.827811",
     "exception": false,
     "start_time": "2025-10-19T18:25:50.402608",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.6.0+cu124)\r\n",
      "Requirement already satisfied: torchvision in /usr/local/lib/python3.11/dist-packages (0.21.0+cu124)\r\n",
      "Requirement already satisfied: pycocotools in /usr/local/lib/python3.11/dist-packages (2.0.10)\r\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (4.67.1)\r\n",
      "Requirement already satisfied: pillow in /usr/local/lib/python3.11/dist-packages (11.3.0)\r\n",
      "Requirement already satisfied: torchmetrics in /usr/local/lib/python3.11/dist-packages (1.8.2)\r\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch) (3.19.1)\r\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch) (4.15.0)\r\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.5)\r\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.6)\r\n",
      "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch) (2025.9.0)\r\n",
      "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch)\r\n",
      "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\r\n",
      "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch)\r\n",
      "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\r\n",
      "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch)\r\n",
      "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\r\n",
      "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch)\r\n",
      "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\r\n",
      "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch)\r\n",
      "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\r\n",
      "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch)\r\n",
      "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\r\n",
      "Collecting nvidia-curand-cu12==10.3.5.147 (from torch)\r\n",
      "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\r\n",
      "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch)\r\n",
      "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\r\n",
      "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch)\r\n",
      "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\r\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch) (0.6.2)\r\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch) (2.21.5)\r\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\r\n",
      "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch)\r\n",
      "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\r\n",
      "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch) (3.2.0)\r\n",
      "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch) (1.13.1)\r\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch) (1.3.0)\r\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from torchvision) (1.26.4)\r\n",
      "Requirement already satisfied: packaging>17.1 in /usr/local/lib/python3.11/dist-packages (from torchmetrics) (25.0)\r\n",
      "Requirement already satisfied: lightning-utilities>=0.8.0 in /usr/local/lib/python3.11/dist-packages (from torchmetrics) (0.15.2)\r\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from lightning-utilities>=0.8.0->torchmetrics) (75.2.0)\r\n",
      "Requirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy->torchvision) (1.3.8)\r\n",
      "Requirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy->torchvision) (1.2.4)\r\n",
      "Requirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy->torchvision) (0.1.1)\r\n",
      "Requirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy->torchvision) (2025.2.0)\r\n",
      "Requirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy->torchvision) (2022.2.0)\r\n",
      "Requirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy->torchvision) (2.4.1)\r\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch) (3.0.2)\r\n",
      "Requirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy->torchvision) (2024.2.0)\r\n",
      "Requirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy->torchvision) (2022.2.0)\r\n",
      "Requirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy->torchvision) (1.4.0)\r\n",
      "Requirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy->torchvision) (2024.2.0)\r\n",
      "Requirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy->torchvision) (2024.2.0)\r\n",
      "Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m74.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m51.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m44.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m28.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m12.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m7.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m71.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hInstalling collected packages: nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12\r\n",
      "  Attempting uninstall: nvidia-nvjitlink-cu12\r\n",
      "    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\r\n",
      "    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\r\n",
      "      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\r\n",
      "  Attempting uninstall: nvidia-curand-cu12\r\n",
      "    Found existing installation: nvidia-curand-cu12 10.3.6.82\r\n",
      "    Uninstalling nvidia-curand-cu12-10.3.6.82:\r\n",
      "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\r\n",
      "  Attempting uninstall: nvidia-cufft-cu12\r\n",
      "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\r\n",
      "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\r\n",
      "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\r\n",
      "  Attempting uninstall: nvidia-cuda-runtime-cu12\r\n",
      "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\r\n",
      "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\r\n",
      "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\r\n",
      "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\r\n",
      "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\r\n",
      "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\r\n",
      "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\r\n",
      "  Attempting uninstall: nvidia-cuda-cupti-cu12\r\n",
      "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\r\n",
      "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\r\n",
      "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\r\n",
      "  Attempting uninstall: nvidia-cublas-cu12\r\n",
      "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\r\n",
      "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\r\n",
      "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\r\n",
      "  Attempting uninstall: nvidia-cusparse-cu12\r\n",
      "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\r\n",
      "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\r\n",
      "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\r\n",
      "  Attempting uninstall: nvidia-cudnn-cu12\r\n",
      "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\r\n",
      "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\r\n",
      "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\r\n",
      "  Attempting uninstall: nvidia-cusolver-cu12\r\n",
      "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\r\n",
      "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\r\n",
      "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\r\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\r\n",
      "libcugraph-cu12 25.6.0 requires libraft-cu12==25.6.*, but you have libraft-cu12 25.2.0 which is incompatible.\r\n",
      "pylibcugraph-cu12 25.6.0 requires pylibraft-cu12==25.6.*, but you have pylibraft-cu12 25.2.0 which is incompatible.\r\n",
      "pylibcugraph-cu12 25.6.0 requires rmm-cu12==25.6.*, but you have rmm-cu12 25.2.0 which is incompatible.\u001b[0m\u001b[31m\r\n",
      "\u001b[0mSuccessfully installed nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "The following NEW packages will be installed:\r\n",
      "  tree\r\n",
      "0 upgraded, 1 newly installed, 0 to remove and 132 not upgraded.\r\n",
      "Need to get 47.9 kB of archives.\r\n",
      "After this operation, 116 kB of additional disk space will be used.\r\n",
      "Get:1 http://archive.ubuntu.com/ubuntu jammy/universe amd64 tree amd64 2.0.2-1 [47.9 kB]\r\n",
      "Fetched 47.9 kB in 0s (106 kB/s)\r\n",
      "\n",
      "\u001b7\u001b[0;23r\u001b8\u001b[1ASelecting previously unselected package tree.\r\n",
      "(Reading database ... 128639 files and directories currently installed.)\r\n",
      "Preparing to unpack .../tree_2.0.2-1_amd64.deb ...\r\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [  0%]\u001b[49m\u001b[39m [..........................................................] \u001b8\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 20%]\u001b[49m\u001b[39m [###########...............................................] \u001b8Unpacking tree (2.0.2-1) ...\r\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 40%]\u001b[49m\u001b[39m [#######################...................................] \u001b8Setting up tree (2.0.2-1) ...\r\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 60%]\u001b[49m\u001b[39m [##################################........................] \u001b8\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 80%]\u001b[49m\u001b[39m [##############################################............] \u001b8Processing triggers for man-db (2.10.2-1) ...\r\n",
      "\r\n",
      "\u001b7\u001b[0;24r\u001b8\u001b[1A\u001b[J"
     ]
    }
   ],
   "source": [
    "!pip install torch torchvision pycocotools tqdm pillow torchmetrics\n",
    "!apt install tree -y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a67b95b6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-19T18:27:17.889342Z",
     "iopub.status.busy": "2025-10-19T18:27:17.889018Z",
     "iopub.status.idle": "2025-10-19T18:27:33.052733Z",
     "shell.execute_reply": "2025-10-19T18:27:33.052083Z"
    },
    "id": "PYYfn33WZQyj",
    "papermill": {
     "duration": 15.195514,
     "end_time": "2025-10-19T18:27:33.054105",
     "exception": false,
     "start_time": "2025-10-19T18:27:17.858591",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Célula 2 (VERIFIQUE SE ESTÁ ASSIM)\n",
    "\n",
    "# Importar bibliotecas\n",
    "import torch\n",
    "import torchvision\n",
    "from torchvision import tv_tensors\n",
    "from torchvision.models.detection import SSD300_VGG16_Weights\n",
    "from torchvision.models.detection.ssd import SSDHead, SSDClassificationHead\n",
    "from torchvision.datasets import CocoDetection\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import os\n",
    "from torchmetrics.detection import MeanAveragePrecision\n",
    "import torchvision.transforms.v2 as T\n",
    "from PIL import Image\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c70d2708",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-19T18:27:33.115502Z",
     "iopub.status.busy": "2025-10-19T18:27:33.114744Z",
     "iopub.status.idle": "2025-10-19T18:27:33.288079Z",
     "shell.execute_reply": "2025-10-19T18:27:33.287163Z"
    },
    "executionInfo": {
     "elapsed": 127,
     "status": "ok",
     "timestamp": 1760821478014,
     "user": {
      "displayName": "Otávio Leite",
      "userId": "01516780190388766182"
     },
     "user_tz": 180
    },
    "id": "NQF2nOoy89Bx",
    "outputId": "56889a5d-ae67-48fe-a17b-139c2308d664",
    "papermill": {
     "duration": 0.205175,
     "end_time": "2025-10-19T18:27:33.289615",
     "exception": false,
     "start_time": "2025-10-19T18:27:33.084440",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[01;34m/kaggle/input/robocup-coco\u001b[0m\r\n",
      "└── \u001b[01;34mrobocup_coco\u001b[0m\r\n",
      "    ├── \u001b[00mREADME.dataset.txt\u001b[0m\r\n",
      "    ├── \u001b[00mREADME.roboflow.txt\u001b[0m\r\n",
      "    ├── \u001b[01;34mtest\u001b[0m\r\n",
      "    │   ├── \u001b[00m_annotations.coco.json\u001b[0m\r\n",
      "    │   └── \u001b[01;34mImagens\u001b[0m\r\n",
      "    ├── \u001b[01;34mtrain\u001b[0m\r\n",
      "    │   ├── \u001b[00m_annotations.coco.json\u001b[0m\r\n",
      "    │   └── \u001b[01;34mImagens\u001b[0m\r\n",
      "    └── \u001b[01;34mvalid\u001b[0m\r\n",
      "        ├── \u001b[00m_annotations.coco.json\u001b[0m\r\n",
      "        └── \u001b[01;34mImagens\u001b[0m\r\n",
      "\r\n",
      "7 directories, 5 files\r\n"
     ]
    }
   ],
   "source": [
    "LOCAL_DATASET_PATH = \"/kaggle/input/robocup-coco\"\n",
    "\n",
    "# (Opcional) Mostra a estrutura descompactada\n",
    "!tree -L 3 \"{LOCAL_DATASET_PATH}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "30927cb1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-19T18:27:33.350289Z",
     "iopub.status.busy": "2025-10-19T18:27:33.349620Z",
     "iopub.status.idle": "2025-10-19T18:27:33.434144Z",
     "shell.execute_reply": "2025-10-19T18:27:33.433203Z"
    },
    "executionInfo": {
     "elapsed": 32,
     "status": "ok",
     "timestamp": 1760821478069,
     "user": {
      "displayName": "Otávio Leite",
      "userId": "01516780190388766182"
     },
     "user_tz": 180
    },
    "id": "qGLYWWuiUC5-",
    "outputId": "70174fd0-34ca-4a8e-c881-ed2a3e371a07",
    "papermill": {
     "duration": 0.115699,
     "end_time": "2025-10-19T18:27:33.435296",
     "exception": false,
     "start_time": "2025-10-19T18:27:33.319597",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Usando o dispositivo: cuda\n",
      "Checkpoints serão salvos em: /kaggle/working/ssd_vgg16_augmented_checkpoint\n"
     ]
    }
   ],
   "source": [
    "# Configurações\n",
    "DATASET_PATH = LOCAL_DATASET_PATH + \"/robocup_coco\"\n",
    "NUM_EPOCHS = 30\n",
    "BATCH_SIZE = 4\n",
    "DEVICE = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "print(f\"Usando o dispositivo: {DEVICE}\")\n",
    "\n",
    "# Configurações checkpointing\n",
    "# Para Colab (aponte para seu Google Drive):\n",
    "# CHECKPOINT_DIR = \"/content/drive/MyDrive/UFU/MSc. 2025-2/Top IA - Análise de Imagem e Vídeo/Colab/Trabalho Final/ssd_vgg16_augmented_checkpoint\"\n",
    "# Para Kaggle (use o diretório de trabalho):\n",
    "CHECKPOINT_DIR = \"/kaggle/working/ssd_vgg16_augmented_checkpoint\"\n",
    "\n",
    "os.makedirs(CHECKPOINT_DIR, exist_ok=True)\n",
    "CHECKPOINT_PATH = os.path.join(CHECKPOINT_DIR, \"checkpoint.pth\")\n",
    "BEST_MODEL_PATH = os.path.join(CHECKPOINT_DIR, \"best_model.pth\")\n",
    "print(f\"Checkpoints serão salvos em: {CHECKPOINT_DIR}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "71f5bb9c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-19T18:27:33.495677Z",
     "iopub.status.busy": "2025-10-19T18:27:33.495374Z",
     "iopub.status.idle": "2025-10-19T18:27:33.500221Z",
     "shell.execute_reply": "2025-10-19T18:27:33.499595Z"
    },
    "id": "c5tcbt6GmcBw",
    "papermill": {
     "duration": 0.036336,
     "end_time": "2025-10-19T18:27:33.501370",
     "exception": false,
     "start_time": "2025-10-19T18:27:33.465034",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Criar modelo\n",
    "def get_model(num_classes):\n",
    "    \"\"\"Cria um modelo SSD300 com backbone VGG16.\"\"\"\n",
    "    \n",
    "    # 1. Carrega o modelo SSD300 com backbone VGG16, pré-treinado no COCO\n",
    "    weights = SSD300_VGG16_Weights.DEFAULT\n",
    "    model = torchvision.models.detection.ssd300_vgg16(weights=weights)\n",
    "\n",
    "    # 2. Obter os parâmetros da cabeça de classificação original\n",
    "    # (Estes são os canais de saída do VGG16 nos quais o SSD faz predições)\n",
    "    in_channels = [512, 1024, 512, 256, 256, 256] \n",
    "    num_anchors = model.anchor_generator.num_anchors_per_location()\n",
    "    \n",
    "    # 3. Criar uma nova cabeça de classificação com o número de classes desejado\n",
    "    # (num_classes já inclui o background)\n",
    "    new_head = SSDClassificationHead(in_channels=in_channels, num_anchors=num_anchors, num_classes=num_classes)\n",
    "\n",
    "    # 4. Substituir a cabeça do modelo\n",
    "    model.head.classification_head = new_head\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b80ca44b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-19T18:27:33.563831Z",
     "iopub.status.busy": "2025-10-19T18:27:33.563032Z",
     "iopub.status.idle": "2025-10-19T18:27:34.947882Z",
     "shell.execute_reply": "2025-10-19T18:27:34.947047Z"
    },
    "papermill": {
     "duration": 1.417353,
     "end_time": "2025-10-19T18:27:34.949384",
     "exception": false,
     "start_time": "2025-10-19T18:27:33.532031",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Copiando checkpoints pré-treinados do Kaggle Input para o Kaggle Working...\n",
      "Arquivo checkpoint.pth copiado.\n",
      "Cópia concluída. Verificando arquivos:\n",
      "total 203164\r\n",
      "-rw-r--r-- 1 root root 208038536 Oct 19 18:27 checkpoint.pth\r\n"
     ]
    }
   ],
   "source": [
    "# 1. Onde seu script de treino espera que os checkpoints estejam\n",
    "# (Este caminho DEVE ser o mesmo da Célula 5)\n",
    "KAGGLE_WORKING_DIR = CHECKPOINT_DIR # CHECKPOINT_DIR já foi definido na Célula 5\n",
    "\n",
    "# 2. Onde o Kaggle montou seu Dataset de checkpoints\n",
    "# (Mude 'robocup-mobilenet-checkpoint' se você deu um nome diferente)\n",
    "INPUT_CHECKPOINT_DIR = \"/kaggle/input/checkpoint-ssd-vgg\"\n",
    "\n",
    "# 3. Os arquivos que queremos copiar\n",
    "CHECKPOINT_FILE = \"checkpoint.pth\"\n",
    "BEST_MODEL_FILE = \"best_model.pth\"\n",
    "\n",
    "# 4. Copia os arquivos\n",
    "print(\"Copiando checkpoints pré-treinados do Kaggle Input para o Kaggle Working...\")\n",
    "\n",
    "# Copia o checkpoint principal\n",
    "src_path = os.path.join(INPUT_CHECKPOINT_DIR, CHECKPOINT_FILE)\n",
    "dst_path = os.path.join(KAGGLE_WORKING_DIR, CHECKPOINT_FILE)\n",
    "if os.path.exists(src_path):\n",
    "    !cp \"{src_path}\" \"{dst_path}\"\n",
    "    print(f\"Arquivo {CHECKPOINT_FILE} copiado.\")\n",
    "else:\n",
    "    print(f\"Aviso: {CHECKPOINT_FILE} não encontrado em {INPUT_CHECKPOINT_DIR}\")\n",
    "\n",
    "# Copia o 'best_model.pth' para que 'best_map' seja carregado corretamente\n",
    "src_path_best = os.path.join(INPUT_CHECKPOINT_DIR, BEST_MODEL_FILE)\n",
    "dst_path_best = os.path.join(KAGGLE_WORKING_DIR, BEST_MODEL_FILE)\n",
    "if os.path.exists(src_path_best):\n",
    "    !cp \"{src_path_best}\" \"{dst_path_best}\"\n",
    "    print(f\"Arquivo {BEST_MODEL_FILE} copiado.\")\n",
    "\n",
    "print(\"Cópia concluída. Verificando arquivos:\")\n",
    "!ls -l \"{KAGGLE_WORKING_DIR}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "203885c0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-19T18:27:35.012604Z",
     "iopub.status.busy": "2025-10-19T18:27:35.012299Z",
     "iopub.status.idle": "2025-10-19T18:27:35.021892Z",
     "shell.execute_reply": "2025-10-19T18:27:35.021355Z"
    },
    "id": "Z8RjZc3MaZY-",
    "papermill": {
     "duration": 0.043561,
     "end_time": "2025-10-19T18:27:35.023081",
     "exception": false,
     "start_time": "2025-10-19T18:27:34.979520",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Médias e desvios padrão do ImageNet (usados pelo VGG16)\n",
    "\n",
    "# Médias e desvios padrão do ImageNet (usados pelo VGG16)\n",
    "MEAN = [0.485, 0.456, 0.406]\n",
    "STD = [0.229, 0.224, 0.225]\n",
    "\n",
    "def get_transform(train):\n",
    "    \"\"\"Define as transformações/data augmentation PARA SSD300+VGG16.\"\"\"\n",
    "    transforms = []\n",
    "    \n",
    "    # --- 1. Augmentations (operam em PIL.Image) ---\n",
    "    if train:\n",
    "        transforms.append(T.RandomHorizontalFlip(0.5))\n",
    "        transforms.append(T.RandomAffine(degrees=10, translate=(0.05, 0.05)))\n",
    "        transforms.append(T.ColorJitter(brightness=0.3, contrast=0.3, saturation=0.3, hue=0.1))\n",
    "        transforms.append(T.GaussianBlur(kernel_size=(5, 9), sigma=(0.1, 5.)))\n",
    "\n",
    "    # --- 2. Conversão (PIL -> Tensor) ---\n",
    "    transforms.append(T.ToImage()) \n",
    "    transforms.append(T.ToDtype(torch.float32, scale=True)) # Normaliza para [0.0, 1.0]\n",
    "\n",
    "    # --- 3. Augmentations (operam em Tensor) ---\n",
    "    if train:\n",
    "        transforms.append(T.RandomErasing(p=0.5, scale=(0.02, 0.2), ratio=(0.3, 3.3)))\n",
    "\n",
    "    # --- 4. Transformações OBRIGATÓRIAS (REINTRODUZIDAS) ---\n",
    "    # O SSD300 espera imagens 300x300\n",
    "    transforms.append(T.Resize([300, 300], antialias=True)) \n",
    "    \n",
    "    # Remove caixas que podem ter ficado inválidas APÓS o resize/affine\n",
    "    transforms.append(T.SanitizeBoundingBoxes()) \n",
    "    \n",
    "    # Normaliza com as médias do VGG16 (necessário para pesos pré-treinados)\n",
    "    transforms.append(T.Normalize(mean=MEAN, std=STD))\n",
    "\n",
    "    return T.Compose(transforms)\n",
    "\n",
    "# Necessária porque, em detecção de objetos, os dados dentro de um lote (batch)\n",
    "# não têm o mesmo tamanho (são \"irregulares\": imagens com tamanhos diferentes, nro. de boxes diferentes).\n",
    "def collate_fn(batch):\n",
    "    \"\"\"\n",
    "    Função 'collate' para o DataLoader, para lidar com lotes de dados.\n",
    "    Lógica personalizada para criar um batch.\n",
    "    \"\"\"\n",
    "    return tuple(zip(*batch))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "79cef513",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-19T18:27:35.085642Z",
     "iopub.status.busy": "2025-10-19T18:27:35.085330Z",
     "iopub.status.idle": "2025-10-19T18:27:35.096830Z",
     "shell.execute_reply": "2025-10-19T18:27:35.096236Z"
    },
    "id": "zEPEnjzHqXuP",
    "papermill": {
     "duration": 0.043925,
     "end_time": "2025-10-19T18:27:35.098029",
     "exception": false,
     "start_time": "2025-10-19T18:27:35.054104",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class RobocupCocoDataset(CocoDetection):\n",
    "    def __init__(self, root, annFile, transforms=None, junk_category_name=None):\n",
    "        # 1. Inicializa a classe CocoDetection\n",
    "        # Não passa as transformações para o 'super' ainda,\n",
    "        # pois as aplicaremos manualmente no final\n",
    "        super().__init__(root, annFile)\n",
    "\n",
    "        # 2. Armazena as transformações\n",
    "        self.transforms = transforms\n",
    "\n",
    "        # 3. Aplica o patch (remove junk classes)\n",
    "        if junk_category_name:\n",
    "            print(f\"Iniciando 'patch' do dataset para remover '{junk_category_name}'...\")\n",
    "            self._patch_dataset(junk_category_name)\n",
    "            print(\"Patch concluído.\")\n",
    "\n",
    "    def _patch_dataset(self, junk_category_name):\n",
    "        junk_cat_id = None\n",
    "        for cat_id, cat_info in list(self.coco.cats.items()):\n",
    "            if cat_info['name'] == junk_category_name:\n",
    "                junk_cat_id = cat_id\n",
    "                del self.coco.cats[cat_id]\n",
    "                break\n",
    "\n",
    "        if junk_cat_id is None:\n",
    "            print(f\"Aviso: Categoria '{junk_category_name}' não encontrada.\")\n",
    "            return\n",
    "\n",
    "        print(f\"Categoria 'junk' encontrada com ID: {junk_cat_id}. Removendo...\")\n",
    "        anns_to_remove = []\n",
    "        for ann_id, ann in self.coco.anns.items():\n",
    "            if ann['category_id'] == junk_cat_id:\n",
    "                anns_to_remove.append(ann_id)\n",
    "\n",
    "        for ann_id in anns_to_remove:\n",
    "            del self.coco.anns[ann_id]\n",
    "\n",
    "        for img_id, ann_ids in self.coco.imgToAnns.items():\n",
    "            self.coco.imgToAnns[img_id] = [ann_id for ann_id in ann_ids if ann_id not in anns_to_remove]\n",
    "\n",
    "        original_image_count = len(self.ids)\n",
    "        self.ids = [img_id for img_id in self.ids if len(self.coco.imgToAnns[img_id]) > 0]\n",
    "        new_image_count = len(self.ids)\n",
    "\n",
    "        print(f\"{len(anns_to_remove)} anotações 'junk' removidas.\")\n",
    "        print(f\"{original_image_count - new_image_count} imagens removidas por ficarem vazias.\")\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        # 1. Pega os dados brutos usando a lógica interna do CocoDetection\n",
    "        img_id = self.ids[index]\n",
    "        image = self._load_image(img_id)      # Carrega a Imagem PIL\n",
    "        target_list = self._load_target(img_id) # Carrega a LISTA de anotações\n",
    "\n",
    "        # 2. Converte a lista de anotações (target_list) no dicionário (target_dict)\n",
    "        boxes = []\n",
    "        labels = []\n",
    "        areas = []\n",
    "        iscrowd = []\n",
    "\n",
    "        for ann in target_list:\n",
    "            # O formato COCO 'bbox' é [x, y, width, height]\n",
    "            # O formato 'torchvision' 'boxes' é [x1, y1, x2, y2]\n",
    "            xmin = ann['bbox'][0]\n",
    "            ymin = ann['bbox'][1]\n",
    "            xmax = xmin + ann['bbox'][2]\n",
    "            ymax = ymin + ann['bbox'][3]\n",
    "            boxes.append([xmin, ymin, xmax, ymax])\n",
    "\n",
    "            labels.append(ann['category_id'])\n",
    "            areas.append(ann['area'])\n",
    "            iscrowd.append(ann['iscrowd'])\n",
    "\n",
    "        # 3. Converte para Tensores\n",
    "        boxes = torch.as_tensor(boxes, dtype=torch.float32)\n",
    "        labels = torch.as_tensor(labels, dtype=torch.int64)\n",
    "        areas = torch.as_tensor(areas, dtype=torch.float32)\n",
    "        iscrowd = torch.as_tensor(iscrowd, dtype=torch.int64)\n",
    "        image_id = torch.tensor([img_id])\n",
    "\n",
    "        # 4. Monta o dicionário de target final\n",
    "        target_dict = {}\n",
    "        \n",
    "        # Envolve o tensor de caixas na classe BoundingBoxes\n",
    "        # Isso informa à API v2 o formato e o tamanho da imagem (canvas)\n",
    "        target_dict[\"boxes\"] = tv_tensors.BoundingBoxes(\n",
    "            boxes,\n",
    "            format=tv_tensors.BoundingBoxFormat.XYXY,\n",
    "            canvas_size=image.size[::-1]  # (height, width)\n",
    "        )\n",
    "        \n",
    "        target_dict[\"labels\"] = labels\n",
    "        target_dict[\"image_id\"] = image_id\n",
    "        target_dict[\"area\"] = areas\n",
    "        target_dict[\"iscrowd\"] = iscrowd\n",
    "\n",
    "        # 5. Aplica as transformações (agora elas recebem a img e o target_dict)\n",
    "        if self.transforms is not None:\n",
    "            image, target_dict = self.transforms(image, target_dict)\n",
    "\n",
    "        return image, target_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1ed2785",
   "metadata": {
    "id": "BJcJuISsBPaV",
    "papermill": {
     "duration": 0.029462,
     "end_time": "2025-10-19T18:27:35.157848",
     "exception": false,
     "start_time": "2025-10-19T18:27:35.128386",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Treinamento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9bdde8a6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-19T18:27:35.220069Z",
     "iopub.status.busy": "2025-10-19T18:27:35.219442Z",
     "iopub.status.idle": "2025-10-19T18:27:36.691674Z",
     "shell.execute_reply": "2025-10-19T18:27:36.690700Z"
    },
    "executionInfo": {
     "elapsed": 1647,
     "status": "ok",
     "timestamp": 1760821479855,
     "user": {
      "displayName": "Otávio Leite",
      "userId": "01516780190388766182"
     },
     "user_tz": 180
    },
    "id": "neQxRuBs-eCi",
    "outputId": "82f501bb-0088-4d30-d5a2-c459dc903049",
    "papermill": {
     "duration": 1.504762,
     "end_time": "2025-10-19T18:27:36.693092",
     "exception": false,
     "start_time": "2025-10-19T18:27:35.188330",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n",
      "Done (t=1.27s)\n",
      "creating index...\n",
      "index created!\n",
      "Iniciando 'patch' do dataset para remover 'Axis2-Bearing2-Housing-Motor2-FRQP'...\n",
      "Categoria 'junk' encontrada com ID: 0. Removendo...\n",
      "0 anotações 'junk' removidas.\n",
      "1960 imagens removidas por ficarem vazias.\n",
      "Patch concluído.\n",
      "loading annotations into memory...\n",
      "Done (t=0.05s)\n",
      "creating index...\n",
      "index created!\n",
      "Iniciando 'patch' do dataset para remover 'Axis2-Bearing2-Housing-Motor2-FRQP'...\n",
      "Categoria 'junk' encontrada com ID: 0. Removendo...\n",
      "0 anotações 'junk' removidas.\n",
      "98 imagens removidas por ficarem vazias.\n",
      "Patch concluído.\n",
      "Dataset de treino carregado com 32489 imagens.\n",
      "Dataset de validação carregado com 1628 imagens.\n",
      "Número de classes detectado: 18 (+1 background = 19)\n",
      "Categorias: \n",
      "{1: {'id': 1, 'name': 'AllenKey', 'supercategory': 'Axis2-Bearing2-Housing-Motor2-FRQP'}, 2: {'id': 2, 'name': 'Axis2', 'supercategory': 'Axis2-Bearing2-Housing-Motor2-FRQP'}, 3: {'id': 3, 'name': 'Bearing2', 'supercategory': 'Axis2-Bearing2-Housing-Motor2-FRQP'}, 4: {'id': 4, 'name': 'Drill', 'supercategory': 'Axis2-Bearing2-Housing-Motor2-FRQP'}, 5: {'id': 5, 'name': 'F20_20_B', 'supercategory': 'Axis2-Bearing2-Housing-Motor2-FRQP'}, 6: {'id': 6, 'name': 'F20_20_G', 'supercategory': 'Axis2-Bearing2-Housing-Motor2-FRQP'}, 7: {'id': 7, 'name': 'Housing', 'supercategory': 'Axis2-Bearing2-Housing-Motor2-FRQP'}, 8: {'id': 8, 'name': 'M20', 'supercategory': 'Axis2-Bearing2-Housing-Motor2-FRQP'}, 9: {'id': 9, 'name': 'M20_100', 'supercategory': 'Axis2-Bearing2-Housing-Motor2-FRQP'}, 10: {'id': 10, 'name': 'M30', 'supercategory': 'Axis2-Bearing2-Housing-Motor2-FRQP'}, 11: {'id': 11, 'name': 'Motor2', 'supercategory': 'Axis2-Bearing2-Housing-Motor2-FRQP'}, 12: {'id': 12, 'name': 'S40_40_B', 'supercategory': 'Axis2-Bearing2-Housing-Motor2-FRQP'}, 13: {'id': 13, 'name': 'S40_40_G', 'supercategory': 'Axis2-Bearing2-Housing-Motor2-FRQP'}, 14: {'id': 14, 'name': 'Screwdriver', 'supercategory': 'Axis2-Bearing2-Housing-Motor2-FRQP'}, 15: {'id': 15, 'name': 'Spacer', 'supercategory': 'Axis2-Bearing2-Housing-Motor2-FRQP'}, 16: {'id': 16, 'name': 'Wrench', 'supercategory': 'Axis2-Bearing2-Housing-Motor2-FRQP'}, 17: {'id': 17, 'name': 'container_box_blue', 'supercategory': 'Axis2-Bearing2-Housing-Motor2-FRQP'}, 18: {'id': 18, 'name': 'container_box_red', 'supercategory': 'Axis2-Bearing2-Housing-Motor2-FRQP'}}\n",
      "Mapa de classes para o modelo:\n",
      "{1: 'AllenKey', 2: 'Axis2', 3: 'Bearing2', 4: 'Drill', 5: 'F20_20_B', 6: 'F20_20_G', 7: 'Housing', 8: 'M20', 9: 'M20_100', 10: 'M30', 11: 'Motor2', 12: 'S40_40_B', 13: 'S40_40_G', 14: 'Screwdriver', 15: 'Spacer', 16: 'Wrench', 17: 'container_box_blue', 18: 'container_box_red'}\n"
     ]
    }
   ],
   "source": [
    "# Caminhos para os conjuntos de treino e validação\n",
    "train_dir = os.path.join(DATASET_PATH, \"train\")\n",
    "valid_dir = os.path.join(DATASET_PATH, \"valid\")\n",
    "\n",
    "# Caminhos específicos para imagens e anotações de TREINO\n",
    "train_img_dir = os.path.join(train_dir, \"Imagens\")\n",
    "train_ann_file = os.path.join(train_dir, \"_annotations.coco.json\")\n",
    "\n",
    "# Caminhos específicos para imagens e anotações de VALIDAÇÃO\n",
    "valid_img_dir = os.path.join(valid_dir, \"Imagens\")\n",
    "valid_ann_file = os.path.join(valid_dir, \"_annotations.coco.json\")\n",
    "\n",
    "JUNK_NAME = 'Axis2-Bearing2-Housing-Motor2-FRQP'\n",
    "\n",
    "# 1. Criar o Dataset de Treino\n",
    "train_dataset = RobocupCocoDataset(\n",
    "    root=train_img_dir,\n",
    "    annFile=train_ann_file,\n",
    "    transforms=get_transform(train=True),\n",
    "    junk_category_name=JUNK_NAME\n",
    ")\n",
    "\n",
    "# 2. Criar o Dataset de Validação\n",
    "valid_dataset = RobocupCocoDataset(\n",
    "    root=valid_img_dir,\n",
    "    annFile=valid_ann_file,\n",
    "    transforms=get_transform(train=False),\n",
    "    junk_category_name=JUNK_NAME\n",
    ")\n",
    "\n",
    "print(f\"Dataset de treino carregado com {len(train_dataset)} imagens.\")\n",
    "print(f\"Dataset de validação carregado com {len(valid_dataset)} imagens.\")\n",
    "\n",
    "num_classes_from_dataset = len(train_dataset.coco.cats)\n",
    "NUM_CLASSES = num_classes_from_dataset + 1 # +1 para o background\n",
    "print(f\"Número de classes detectado: {num_classes_from_dataset} (+1 background = {NUM_CLASSES})\")\n",
    "\n",
    "# Criando o mapa de classes para usar na inferência (opcional, mas útil)\n",
    "CLASS_NAMES = {cat_id: info['name'] for cat_id, info in train_dataset.coco.cats.items()}\n",
    "\n",
    "print(\"Categorias: \")\n",
    "print(train_dataset.coco.cats)\n",
    "model_class_map = {i: cat['name'] for i, cat in train_dataset.coco.cats.items()}\n",
    "print(\"Mapa de classes para o modelo:\")\n",
    "print(model_class_map)\n",
    "\n",
    "# --- DATALOADERS ---\n",
    "train_loader = DataLoader(\n",
    "    train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=2, collate_fn=collate_fn\n",
    ")\n",
    "valid_loader = DataLoader(\n",
    "    valid_dataset, batch_size=1, shuffle=False, num_workers=2, collate_fn=collate_fn\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c10daa6c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-19T18:27:36.759946Z",
     "iopub.status.busy": "2025-10-19T18:27:36.759127Z",
     "iopub.status.idle": "2025-10-19T18:27:41.247852Z",
     "shell.execute_reply": "2025-10-19T18:27:41.247206Z"
    },
    "id": "vhkze9AMBU2C",
    "papermill": {
     "duration": 4.523844,
     "end_time": "2025-10-19T18:27:41.249253",
     "exception": false,
     "start_time": "2025-10-19T18:27:36.725409",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: \"https://download.pytorch.org/models/ssd300_vgg16_coco-b556d3b4.pth\" to /root/.cache/torch/hub/checkpoints/ssd300_vgg16_coco-b556d3b4.pth\n",
      "100%|██████████| 136M/136M [00:00<00:00, 176MB/s]\n"
     ]
    }
   ],
   "source": [
    "# Defina o número de classes (suas classes + 1 para o fundo)\n",
    "model = get_model(NUM_CLASSES)\n",
    "\n",
    "# Mover o modelo para a GPU, se disponível\n",
    "model.to(DEVICE)\n",
    "\n",
    "# Definir otimizador\n",
    "params = [p for p in model.parameters() if p.requires_grad]\n",
    "optimizer = torch.optim.SGD(params, lr=0.005, momentum=0.9, weight_decay=0.0005)\n",
    "\n",
    "# Agendador de taxa de aprendizado\n",
    "lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=3, gamma=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f514373c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-19T18:27:41.313789Z",
     "iopub.status.busy": "2025-10-19T18:27:41.313022Z",
     "iopub.status.idle": "2025-10-19T18:27:41.460593Z",
     "shell.execute_reply": "2025-10-19T18:27:41.459700Z"
    },
    "executionInfo": {
     "elapsed": 51,
     "status": "ok",
     "timestamp": 1760821480900,
     "user": {
      "displayName": "Otávio Leite",
      "userId": "01516780190388766182"
     },
     "user_tz": 180
    },
    "id": "DmSjICaepqzT",
    "outputId": "d8cc7683-382a-4aa7-ecb4-1e276652df78",
    "papermill": {
     "duration": 0.180818,
     "end_time": "2025-10-19T18:27:41.461975",
     "exception": false,
     "start_time": "2025-10-19T18:27:41.281157",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Carregando checkpoint de '/kaggle/working/ssd_vgg16_augmented_checkpoint/checkpoint.pth'...\n",
      "Checkpoint carregado. Resumindo da época 24\n",
      "Melhor mAP até agora: 0.0000\n"
     ]
    }
   ],
   "source": [
    "# --- LÓGICA PARA CARREGAR O CHECKPOINT (RESUMIR) ---\n",
    "start_epoch = 0\n",
    "best_map = 0.0\n",
    "\n",
    "# Inicializa listas para armazenar o histórico\n",
    "train_loss_history = []\n",
    "val_map_history = []\n",
    "\n",
    "if os.path.exists(CHECKPOINT_PATH):\n",
    "    print(f\"Carregando checkpoint de '{CHECKPOINT_PATH}'...\")\n",
    "    checkpoint = torch.load(CHECKPOINT_PATH, map_location=DEVICE)\n",
    "\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "    lr_scheduler.load_state_dict(checkpoint['scheduler_state_dict'])\n",
    "    start_epoch = checkpoint['epoch'] + 1 # Começa da *próxima* época\n",
    "    best_map = checkpoint['best_map']\n",
    "\n",
    "    # Carrega o histórico salvo para continuar\n",
    "    if 'train_loss_history' in checkpoint:\n",
    "        train_loss_history = checkpoint['train_loss_history']\n",
    "    if 'val_map_history' in checkpoint:\n",
    "        val_map_history = checkpoint['val_map_history']\n",
    "\n",
    "    print(f\"Checkpoint carregado. Resumindo da época {start_epoch}\")\n",
    "    print(f\"Melhor mAP até agora: {best_map:.4f}\")\n",
    "else:\n",
    "    print(\"Nenhum checkpoint encontrado. Começando treinamento do zero.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "72a25534",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-19T18:27:41.526134Z",
     "iopub.status.busy": "2025-10-19T18:27:41.525834Z",
     "iopub.status.idle": "2025-10-19T21:28:26.688808Z",
     "shell.execute_reply": "2025-10-19T21:28:26.687993Z"
    },
    "id": "jwzlBb6vCBKH",
    "outputId": "d2d550ff-6110-4a55-d509-1ee98d2340f0",
    "papermill": {
     "duration": 10848.93263,
     "end_time": "2025-10-19T21:28:30.425925",
     "exception": false,
     "start_time": "2025-10-19T18:27:41.493295",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Época 25/30:   0%|          | 0/8123 [00:00<?, ?batch/s]/usr/local/lib/python3.11/dist-packages/torchvision/transforms/v2/_augment.py:93: UserWarning: RandomErasing() is currently passing through inputs of type tv_tensors.BoundingBoxes. This will likely change in the future.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/torchvision/transforms/v2/_augment.py:93: UserWarning: RandomErasing() is currently passing through inputs of type tv_tensors.BoundingBoxes. This will likely change in the future.\n",
      "  warnings.warn(\n",
      "Época 25/30: 100%|██████████| 8123/8123 [26:26<00:00,  5.12batch/s, loss=nan]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Época 25/30, Perda: nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Métricas de Validação:\n",
      "  map: 0.0000\n",
      "  map_50: 0.0000\n",
      "  map_75: 0.0000\n",
      "  map_small: 0.0000\n",
      "  map_medium: 0.0000\n",
      "  map_large: 0.0000\n",
      "  mar_1: 0.0000\n",
      "  mar_10: 0.0000\n",
      "  mar_100: 0.0000\n",
      "  mar_small: 0.0000\n",
      "  mar_medium: 0.0000\n",
      "  mar_large: 0.0000\n",
      "  map_per_class: -1.0000\n",
      "  mar_100_per_class: -1.0000\n",
      "  classes: tensor([ 1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17, 18],\n",
      "       dtype=torch.int32)\n",
      "Época 25/30 - Perda Treino: nan - mAP Validação: 0.0000\n",
      "Checkpoint da época 25 salvo em '/kaggle/working/ssd_vgg16_augmented_checkpoint/checkpoint.pth'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Época 26/30:   0%|          | 0/8123 [00:00<?, ?batch/s]/usr/local/lib/python3.11/dist-packages/torchvision/transforms/v2/_augment.py:93: UserWarning: RandomErasing() is currently passing through inputs of type tv_tensors.BoundingBoxes. This will likely change in the future.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/torchvision/transforms/v2/_augment.py:93: UserWarning: RandomErasing() is currently passing through inputs of type tv_tensors.BoundingBoxes. This will likely change in the future.\n",
      "  warnings.warn(\n",
      "Época 26/30: 100%|██████████| 8123/8123 [31:07<00:00,  4.35batch/s, loss=nan]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Época 26/30, Perda: nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Métricas de Validação:\n",
      "  map: 0.0000\n",
      "  map_50: 0.0000\n",
      "  map_75: 0.0000\n",
      "  map_small: 0.0000\n",
      "  map_medium: 0.0000\n",
      "  map_large: 0.0000\n",
      "  mar_1: 0.0000\n",
      "  mar_10: 0.0000\n",
      "  mar_100: 0.0000\n",
      "  mar_small: 0.0000\n",
      "  mar_medium: 0.0000\n",
      "  mar_large: 0.0000\n",
      "  map_per_class: -1.0000\n",
      "  mar_100_per_class: -1.0000\n",
      "  classes: tensor([ 1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17, 18],\n",
      "       dtype=torch.int32)\n",
      "Época 26/30 - Perda Treino: nan - mAP Validação: 0.0000\n",
      "Checkpoint da época 26 salvo em '/kaggle/working/ssd_vgg16_augmented_checkpoint/checkpoint.pth'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Época 27/30:   0%|          | 0/8123 [00:00<?, ?batch/s]/usr/local/lib/python3.11/dist-packages/torchvision/transforms/v2/_augment.py:93: UserWarning: RandomErasing() is currently passing through inputs of type tv_tensors.BoundingBoxes. This will likely change in the future.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/torchvision/transforms/v2/_augment.py:93: UserWarning: RandomErasing() is currently passing through inputs of type tv_tensors.BoundingBoxes. This will likely change in the future.\n",
      "  warnings.warn(\n",
      "Época 27/30: 100%|██████████| 8123/8123 [28:58<00:00,  4.67batch/s, loss=nan]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Época 27/30, Perda: nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Métricas de Validação:\n",
      "  map: 0.0000\n",
      "  map_50: 0.0000\n",
      "  map_75: 0.0000\n",
      "  map_small: 0.0000\n",
      "  map_medium: 0.0000\n",
      "  map_large: 0.0000\n",
      "  mar_1: 0.0000\n",
      "  mar_10: 0.0000\n",
      "  mar_100: 0.0000\n",
      "  mar_small: 0.0000\n",
      "  mar_medium: 0.0000\n",
      "  mar_large: 0.0000\n",
      "  map_per_class: -1.0000\n",
      "  mar_100_per_class: -1.0000\n",
      "  classes: tensor([ 1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17, 18],\n",
      "       dtype=torch.int32)\n",
      "Época 27/30 - Perda Treino: nan - mAP Validação: 0.0000\n",
      "Checkpoint da época 27 salvo em '/kaggle/working/ssd_vgg16_augmented_checkpoint/checkpoint.pth'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Época 28/30:   0%|          | 0/8123 [00:00<?, ?batch/s]/usr/local/lib/python3.11/dist-packages/torchvision/transforms/v2/_augment.py:93: UserWarning: RandomErasing() is currently passing through inputs of type tv_tensors.BoundingBoxes. This will likely change in the future.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/torchvision/transforms/v2/_augment.py:93: UserWarning: RandomErasing() is currently passing through inputs of type tv_tensors.BoundingBoxes. This will likely change in the future.\n",
      "  warnings.warn(\n",
      "Época 28/30: 100%|██████████| 8123/8123 [27:21<00:00,  4.95batch/s, loss=nan]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Época 28/30, Perda: nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Métricas de Validação:\n",
      "  map: 0.0000\n",
      "  map_50: 0.0000\n",
      "  map_75: 0.0000\n",
      "  map_small: 0.0000\n",
      "  map_medium: 0.0000\n",
      "  map_large: 0.0000\n",
      "  mar_1: 0.0000\n",
      "  mar_10: 0.0000\n",
      "  mar_100: 0.0000\n",
      "  mar_small: 0.0000\n",
      "  mar_medium: 0.0000\n",
      "  mar_large: 0.0000\n",
      "  map_per_class: -1.0000\n",
      "  mar_100_per_class: -1.0000\n",
      "  classes: tensor([ 1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17, 18],\n",
      "       dtype=torch.int32)\n",
      "Época 28/30 - Perda Treino: nan - mAP Validação: 0.0000\n",
      "Checkpoint da época 28 salvo em '/kaggle/working/ssd_vgg16_augmented_checkpoint/checkpoint.pth'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Época 29/30:   0%|          | 0/8123 [00:00<?, ?batch/s]/usr/local/lib/python3.11/dist-packages/torchvision/transforms/v2/_augment.py:93: UserWarning: RandomErasing() is currently passing through inputs of type tv_tensors.BoundingBoxes. This will likely change in the future.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/torchvision/transforms/v2/_augment.py:93: UserWarning: RandomErasing() is currently passing through inputs of type tv_tensors.BoundingBoxes. This will likely change in the future.\n",
      "  warnings.warn(\n",
      "Época 29/30: 100%|██████████| 8123/8123 [30:54<00:00,  4.38batch/s, loss=nan]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Época 29/30, Perda: nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Métricas de Validação:\n",
      "  map: 0.0000\n",
      "  map_50: 0.0000\n",
      "  map_75: 0.0000\n",
      "  map_small: 0.0000\n",
      "  map_medium: 0.0000\n",
      "  map_large: 0.0000\n",
      "  mar_1: 0.0000\n",
      "  mar_10: 0.0000\n",
      "  mar_100: 0.0000\n",
      "  mar_small: 0.0000\n",
      "  mar_medium: 0.0000\n",
      "  mar_large: 0.0000\n",
      "  map_per_class: -1.0000\n",
      "  mar_100_per_class: -1.0000\n",
      "  classes: tensor([ 1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17, 18],\n",
      "       dtype=torch.int32)\n",
      "Época 29/30 - Perda Treino: nan - mAP Validação: 0.0000\n",
      "Checkpoint da época 29 salvo em '/kaggle/working/ssd_vgg16_augmented_checkpoint/checkpoint.pth'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Época 30/30:   0%|          | 0/8123 [00:00<?, ?batch/s]/usr/local/lib/python3.11/dist-packages/torchvision/transforms/v2/_augment.py:93: UserWarning: RandomErasing() is currently passing through inputs of type tv_tensors.BoundingBoxes. This will likely change in the future.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/torchvision/transforms/v2/_augment.py:93: UserWarning: RandomErasing() is currently passing through inputs of type tv_tensors.BoundingBoxes. This will likely change in the future.\n",
      "  warnings.warn(\n",
      "Época 30/30: 100%|██████████| 8123/8123 [31:49<00:00,  4.25batch/s, loss=nan]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Época 30/30, Perda: nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Métricas de Validação:\n",
      "  map: 0.0000\n",
      "  map_50: 0.0000\n",
      "  map_75: 0.0000\n",
      "  map_small: 0.0000\n",
      "  map_medium: 0.0000\n",
      "  map_large: 0.0000\n",
      "  mar_1: 0.0000\n",
      "  mar_10: 0.0000\n",
      "  mar_100: 0.0000\n",
      "  mar_small: 0.0000\n",
      "  mar_medium: 0.0000\n",
      "  mar_large: 0.0000\n",
      "  map_per_class: -1.0000\n",
      "  mar_100_per_class: -1.0000\n",
      "  classes: tensor([ 1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17, 18],\n",
      "       dtype=torch.int32)\n",
      "Época 30/30 - Perda Treino: nan - mAP Validação: 0.0000\n",
      "Checkpoint da época 30 salvo em '/kaggle/working/ssd_vgg16_augmented_checkpoint/checkpoint.pth'\n",
      "Treinamento concluído.\n",
      "Modelo salvo\n"
     ]
    }
   ],
   "source": [
    "# Loop de treinamento\n",
    "for epoch in range(start_epoch, NUM_EPOCHS):\n",
    "    model.train() # Coloca o modelo em modo de treinamento\n",
    "    epoch_loss = 0\n",
    "\n",
    "    progress_bar = tqdm(train_loader, desc=f\"Época {epoch+1}/{NUM_EPOCHS}\", unit=\"batch\")\n",
    "\n",
    "    for images, targets in progress_bar:\n",
    "        # Mover dados para o device correto\n",
    "        images = list(image.to(DEVICE) for image in images)\n",
    "\n",
    "        # 'targets' é um tuple de dicionários\n",
    "        # Precisamos converter o 'tv_tensor' de volta para 'tensor'\n",
    "        \n",
    "        targets_list = []\n",
    "        for t_dict in targets: # t_dict é o target de UMA imagem\n",
    "            processed_dict = {}\n",
    "            \n",
    "            for k, v in t_dict.items():\n",
    "                if k == \"boxes\":\n",
    "                    # --- Processa \"boxes\" ---\n",
    "                    # O modelo em modo .train() espera o tensor puro (.data)\n",
    "                    if isinstance(v, tv_tensors.BoundingBoxes):\n",
    "                        processed_dict[k] = v.data.to(device=DEVICE, dtype=torch.float32)\n",
    "                    elif isinstance(v, torch.Tensor):\n",
    "                         # Fallback: Se já for tensor\n",
    "                         processed_dict[k] = v.to(device=DEVICE, dtype=torch.float32)\n",
    "                \n",
    "                elif isinstance(v, torch.Tensor):\n",
    "                    # --- Processa TODAS as outras chaves ---\n",
    "                    # Copia 'labels', 'area', 'iscrowd', 'image_id' etc. para a GPU\n",
    "                    processed_dict[k] = v.to(device=DEVICE)\n",
    "                \n",
    "                # Outros tipos (ex: metadados que não são tensores) são ignorados\n",
    "            \n",
    "            # Adiciona à lista apenas se tiver as chaves essenciais\n",
    "            if \"boxes\" in processed_dict and \"labels\" in processed_dict:\n",
    "                targets_list.append(processed_dict)\n",
    "            else:\n",
    "                img_id_val = t_dict.get(\"image_id\", \"Desconhecido\")\n",
    "                if isinstance(img_id_val, torch.Tensor):\n",
    "                    img_id_val = img_id_val.item()\n",
    "                print(f\"Aviso: Target ignorado por falta de 'boxes' ou 'labels' após processamento. Img ID: {img_id_val}\")\n",
    "                \n",
    "        targets = targets_list\n",
    "        # print(targets_list)\n",
    "        # O modelo retorna um dicionário de perdas em modo de treino\n",
    "        loss_dict = model(images, targets)\n",
    "\n",
    "        # Somar todas as perdas\n",
    "        losses = sum(loss for loss in loss_dict.values())\n",
    "\n",
    "        # Backpropagation\n",
    "        optimizer.zero_grad()\n",
    "        losses.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        batch_loss = losses.item()\n",
    "        epoch_loss += batch_loss\n",
    "        progress_bar.set_postfix(loss=batch_loss)\n",
    "\n",
    "    # Calcula e salva a perda da época\n",
    "    train_loss = epoch_loss/len(train_loader)\n",
    "    train_loss_history.append(train_loss)\n",
    "\n",
    "    # Atualiza o agendador de taxa de aprendizado\n",
    "    lr_scheduler.step()\n",
    "\n",
    "    print(f\"Época {epoch+1}/{NUM_EPOCHS}, Perda: {epoch_loss/len(train_loader)}\")\n",
    "\n",
    "    # --- VALIDAÇÃO (ESSENCIAL PARA SALVAR O MELHOR MODELO) ---\n",
    "    model.eval()\n",
    "    metric = MeanAveragePrecision()\n",
    "    with torch.no_grad():\n",
    "        for images, targets in valid_loader:\n",
    "            images = list(image.to(DEVICE) for image in images)\n",
    "            \n",
    "            # --- CORREÇÃO: Processar targets de validação manualmente ---\n",
    "            targets_list_val = []\n",
    "            for t_dict in targets:\n",
    "                processed_dict = {}\n",
    "                for k, v in t_dict.items():\n",
    "                    if k == \"boxes\":\n",
    "                        if isinstance(v, tv_tensors.BoundingBoxes):\n",
    "                            # A métrica espera um tensor puro (.data)\n",
    "                            processed_dict[k] = v.data.to(device=DEVICE, dtype=torch.float32)\n",
    "                        elif isinstance(v, torch.Tensor):\n",
    "                             processed_dict[k] = v.to(device=DEVICE, dtype=torch.float32)\n",
    "                    elif isinstance(v, torch.Tensor):\n",
    "                        # Copia 'labels', 'area', 'iscrowd', 'image_id' etc.\n",
    "                        processed_dict[k] = v.to(device=DEVICE)\n",
    "                targets_list_val.append(processed_dict)\n",
    "            \n",
    "            targets = targets_list_val # Usa a lista processada\n",
    "            predictions = model(images)\n",
    "            metric.update(predictions, targets)\n",
    "\n",
    "    val_metrics = metric.compute()\n",
    "    val_map = val_metrics['map'].item() # Pega o mAP principal (IoU de 0.5 a 0.95)\n",
    "    # Salva a métrica de validação da época\n",
    "    val_map_history.append(val_map)\n",
    "\n",
    "    # Imprime o dicionário completo de métricas\n",
    "    print(\"Métricas de Validação:\")\n",
    "    for k, v in val_metrics.items():\n",
    "        # 'v' é sempre um tensor. Verificamos se ele tem 1 elemento (escalar)\n",
    "        if v.numel() == 1:\n",
    "            # Se for escalar, usamos .item() e formatamos\n",
    "            print(f\"  {k}: {v.item():.4f}\")\n",
    "        else:\n",
    "            # Se tiver >1 elemento (ex: as métricas 'per_class' de 18 elementos)\n",
    "            # Apenas imprimimos o tensor. Não usamos .item()\n",
    "            print(f\"  {k}: {v}\")\n",
    "\n",
    "\n",
    "    print(f\"Época {epoch+1}/{NUM_EPOCHS} - \"\n",
    "            f\"Perda Treino: {train_loss:.4f} - \"\n",
    "            f\"mAP Validação: {val_map:.4f}\")\n",
    "    # --- LÓGICA PARA SALVAR O CHECKPOINT ---\n",
    "\n",
    "    # 1. Salvar o melhor modelo (para inferência futura)\n",
    "    if val_map > best_map:\n",
    "        best_map = val_map\n",
    "        torch.save(model.state_dict(), BEST_MODEL_PATH)\n",
    "        print(f\"Novo melhor modelo salvo com mAP: {best_map:.4f} em '{BEST_MODEL_PATH}'\")\n",
    "\n",
    "    # 2. Salvar o checkpoint da última época (para resumir)\n",
    "    checkpoint_data = {\n",
    "        'epoch': epoch,\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict(),\n",
    "        'scheduler_state_dict': lr_scheduler.state_dict(),\n",
    "        'best_map': best_map,\n",
    "        'train_loss_history': train_loss_history,\n",
    "        'val_map_history': val_map_history\n",
    "    }\n",
    "    torch.save(checkpoint_data, CHECKPOINT_PATH)\n",
    "    print(f\"Checkpoint da época {epoch+1} salvo em '{CHECKPOINT_PATH}'\")\n",
    "\n",
    "print(\"Treinamento concluído.\")\n",
    "\n",
    "# Salvar o modelo treinado\n",
    "torch.save(model.state_dict(), 'ssd_vgg.pth')\n",
    "print(\"Modelo salvo\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcd17668",
   "metadata": {
    "id": "xJu38cg7z1Eu",
    "papermill": {
     "duration": 4.203133,
     "end_time": "2025-10-19T21:28:38.857322",
     "exception": false,
     "start_time": "2025-10-19T21:28:34.654189",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Inferência (e FPS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1d02a1b5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-19T21:28:47.127371Z",
     "iopub.status.busy": "2025-10-19T21:28:47.126852Z",
     "iopub.status.idle": "2025-10-19T21:28:50.013120Z",
     "shell.execute_reply": "2025-10-19T21:28:50.012053Z"
    },
    "id": "fT0Hv7gkz4cQ",
    "papermill": {
     "duration": 7.060679,
     "end_time": "2025-10-19T21:28:50.014291",
     "exception": true,
     "start_time": "2025-10-19T21:28:42.953612",
     "status": "failed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Carregando o melhor modelo de '/kaggle/working/ssd_vgg16_augmented_checkpoint/best_model.pth' para medição de FPS...\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/kaggle/working/ssd_vgg16_augmented_checkpoint/best_model.pth'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_19/3502195726.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;31m# 2. Carregar os pesos salvos\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_state_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mBEST_MODEL_PATH\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmap_location\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mDEVICE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDEVICE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# MUITO IMPORTANTE: colocar em modo de avaliação\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(f, map_location, pickle_module, weights_only, mmap, **pickle_load_args)\u001b[0m\n\u001b[1;32m   1423\u001b[0m         \u001b[0mpickle_load_args\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"encoding\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"utf-8\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1424\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1425\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0m_open_file_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"rb\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mopened_file\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1426\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0m_is_zipfile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopened_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1427\u001b[0m             \u001b[0;31m# The zipfile reader is going to advance the current file position.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36m_open_file_like\u001b[0;34m(name_or_buffer, mode)\u001b[0m\n\u001b[1;32m    749\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_open_file_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    750\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0m_is_path\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname_or_buffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 751\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_open_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    752\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    753\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;34m\"w\"\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, name, mode)\u001b[0m\n\u001b[1;32m    730\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0m_open_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_opener\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    731\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 732\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    733\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    734\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__exit__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/kaggle/working/ssd_vgg16_augmented_checkpoint/best_model.pth'"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "# --- Carregar o MELHOR modelo salvo ---\n",
    "print(f\"Carregando o melhor modelo de '{BEST_MODEL_PATH}' para medição de FPS...\")\n",
    "# 1. Recriar a arquitetura do modelo\n",
    "model = get_model(NUM_CLASSES)\n",
    "\n",
    "# 2. Carregar os pesos salvos\n",
    "model.load_state_dict(torch.load(BEST_MODEL_PATH, map_location=DEVICE))\n",
    "model.to(DEVICE)\n",
    "model.eval() # MUITO IMPORTANTE: colocar em modo de avaliação\n",
    "\n",
    "print(\"Modelo carregado. Iniciando medição de FPS...\")\n",
    "\n",
    "# --- Loop de Medição ---\n",
    "# Usaremos o valid_loader, mas sem calcular métricas, apenas o tempo\n",
    "total_time = 0\n",
    "image_count = 0\n",
    "inference_loader = DataLoader(\n",
    "    valid_dataset, batch_size=1, shuffle=False, num_workers=2, collate_fn=collate_fn\n",
    ")\n",
    "\n",
    "with torch.no_grad():\n",
    "    for images, targets in inference_loader:\n",
    "        images = list(image.to(DEVICE) for image in images)\n",
    "\n",
    "        # Sincroniza a GPU para uma medição de tempo precisa (se estiver usando cuda)\n",
    "        if DEVICE.type == 'cuda':\n",
    "            torch.cuda.synchronize()\n",
    "\n",
    "        start_time = time.time()\n",
    "\n",
    "        # Apenas executa a inferência\n",
    "        predictions = model(images)\n",
    "\n",
    "        if DEVICE.type == 'cuda':\n",
    "            torch.cuda.synchronize()\n",
    "\n",
    "        end_time = time.time()\n",
    "\n",
    "        total_time += (end_time - start_time)\n",
    "        image_count += len(images)\n",
    "\n",
    "# --- Calcular Resultados ---\n",
    "avg_inference_time = total_time / image_count\n",
    "fps = 1.0 / avg_inference_time\n",
    "\n",
    "print(\"--- Resultados de Performance ---\")\n",
    "print(f\"Total de imagens processadas: {image_count}\")\n",
    "print(f\"Tempo total de inferência: {total_time:.2f} segundos\")\n",
    "print(f\"Tempo médio por imagem: {avg_inference_time * 1000:.2f} ms\") # Converte para milissegundos\n",
    "print(f\"FPS (Frames Per Second): {fps:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a66ab1a",
   "metadata": {
    "id": "t7c4l0sj3FiP",
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "# Gráficos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4ad1b5f",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-10-19T05:45:00.302604Z",
     "iopub.status.idle": "2025-10-19T05:45:00.302814Z",
     "shell.execute_reply": "2025-10-19T05:45:00.302724Z",
     "shell.execute_reply.started": "2025-10-19T05:45:00.302715Z"
    },
    "id": "4fw_8ZuK3JRC",
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Carrega o checkpoint final para garantir que temos o histórico completo\n",
    "# (Mesmo que o treinamento tenha acabado de rodar, é uma boa prática)\n",
    "final_checkpoint = torch.load(CHECKPOINT_PATH)\n",
    "loss_hist = final_checkpoint['train_loss_history']\n",
    "map_hist = final_checkpoint['val_map_history']\n",
    "\n",
    "# Cria a figura e o primeiro eixo (Perda de Treino)\n",
    "fig, ax1 = plt.subplots(figsize=(12, 6))\n",
    "\n",
    "# Plot da Perda de Treino\n",
    "color = 'tab:blue'\n",
    "ax1.set_xlabel('Época')\n",
    "ax1.set_ylabel('Perda de Treino (Loss)', color=color)\n",
    "ax1.plot(loss_hist, color=color, label='Perda de Treino')\n",
    "ax1.tick_params(axis='y', labelcolor=color)\n",
    "\n",
    "# Cria o segundo eixo (mAP de Validação) que compartilha o eixo x\n",
    "ax2 = ax1.twinx()\n",
    "color = 'tab:red'\n",
    "ax2.set_ylabel('mAP de Validação', color=color)\n",
    "ax2.plot(map_hist, color=color, label='mAP de Validação')\n",
    "ax2.tick_params(axis='y', labelcolor=color)\n",
    "\n",
    "# Título e legenda\n",
    "plt.title('Performance do Modelo: Perda de Treino vs. mAP de Validação')\n",
    "fig.tight_layout()\n",
    "fig.legend(loc=\"upper right\", bbox_to_anchor=(1,1), bbox_transform=ax1.transAxes)\n",
    "\n",
    "# Salva o gráfico no seu Drive\n",
    "plt.savefig(os.path.join(CHECKPOINT_DIR, \"loss_vs_map_plot.png\"))\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyMMUQB4PPPkVscA4bnrmdGV",
   "gpuType": "T4",
   "provenance": []
  },
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 8520886,
     "sourceId": 13424993,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 8528360,
     "sourceId": 13436230,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31154,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 10990.114759,
   "end_time": "2025-10-19T21:28:56.553066",
   "environment_variables": {},
   "exception": true,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-10-19T18:25:46.438307",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
